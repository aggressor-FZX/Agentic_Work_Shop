# Unified Plan for a Cost-Aware, Asynchronous Job Ingestion Architecture

## Consolidated Document

This document combines the PDF and DOCX versions, with DOCX corrections applied.

---

Unified Plan for a Cost-Aware, Asynchronous Job Ingestion Architecture
Overview and Motivation
The Unified Plan for a Cost-Aware, Asynchronous Job Ingestion Architecture is a comprehensive redesign of the job matching system to address cost, scalability, and performance challenges. The previous synchronous design – where each user search triggered real-time scraping and scoring – proved incompatible with external API rate limits, credit costs, and high latencies[1]. In particular, the integration with the HasData job data provider was driving unpredictable costs due to unbounded API calls and background polling. This new architecture embraces an asynchronous, event-driven pipeline that separates cheap bulk data discovery from expensive enrichment. By doing so, it pre-computes and caches results ahead of time and serves user queries from this cache wherever possible, dramatically reducing live API calls. All external interactions are tightly budgeted and controlled to fit within HasData’s Startup Plan limits (200,000 API credits per month, max 15 concurrent requests). The motivation is to ensure a sustainable platform that provides fast results to users while staying cost-aware and respecting rate limits.
Key strategic decisions guide this plan:
User-Triggered Work Only: No continuous background scraping or periodic “fresh search” jobs are run. All costly operations (like calling HasData) happen only in response to a user action, ensuring we “pay” only when necessary. This avoids the scenario of burning credits on jobs no one looks at.
High Cache Reuse: Job data fetched once is cached and reused for many queries. This yields a high cache hit rate (targeting >90%) and drastically lowers incremental cost per query[2][3]. A cached job listing can be served to free users without incurring new API calls.
Asynchronous Pipeline: Expensive processing (full-text retrieval, embedding, ML scoring) is decoupled from user requests. Instead of blocking a user while we call external APIs, these steps run asynchronously in background workers. The system returns initial results quickly (from cached data or partial info) and then updates or refines results when enrichment completes.
Cost Governance: A centralized cost control mechanism monitors every external API call. We enforce the 200k credits/month budget with real-time tracking, alerts at thresholds, and a hard cut-off (circuit breaker) if the budget would be exceeded[4][5]. Similarly, the 15 concurrent call limit is upheld via a distributed rate limiter.
User Tier Differentiation: The system’s behavior differs for free vs. subscribed users to balance cost and experience. Free users are limited to cached data (no new scraping), whereas paid subscribers can trigger fresh data ingestion when cached results are insufficient. This ensures paid users get comprehensive results while free usage remains essentially cost-free to the platform.
Overall, this unified architecture transforms the job search platform into a cost-efficient, scalable, and responsive system. It replaces the prior approach entirely, introducing new components and workflows (distributed caching, vector search, batch enrichment, etc.) and refactoring old ones (matching engine, ingestion logic) to align with the asynchronous model[6][7]. In the sections below, we detail each aspect of the design, from how searches are initiated, to how data flows through discovery, caching, ranking, and enrichment, and how we enforce usage limits at every step.
Triggering and Orchestration Model
Frontend-Centric Orchestration: In this architecture, the frontend client (e.g. the web application) is the sole orchestrator of multi-step flows[8]. All major actions are initiated by explicit user interactions on the frontend, which then makes targeted API calls to backend services. There are no hidden backend-to-backend calls where one server component silently invokes another without the frontend’s knowledge[9]. This design provides complete transparency and control: the client coordinates the sequence (e.g. “parse resume” → “search jobs” → “enrich details”), and it can decide when to proceed or abort based on intermediate results.
User-Initiated Search: The primary trigger is a user performing a job search, typically by uploading a resume or entering a query. For example, a user uploads their resume and clicks “Find Jobs”. The frontend will then:
Resume Ingestion: Call the Document Reader Service (via an API endpoint) to parse the uploaded resume. The Document Reader extracts plain text and key fields from the resume (including OCR if needed for scanned PDFs). This yields the user’s raw profile data (text of experiences, skills, etc.). The parsed resume text is returned to the client/frontend. (In some implementations, this step could also be done client-side if feasible, but here we treat it as a service call.)
Resume Analysis: Next, the frontend calls the Hermes Service with the extracted resume data. Hermes performs ML-based analysis: it may use an SVM model to predict or categorize the candidate’s job title and expertise, extract structured skills, and augment the data with domain knowledge (e.g. mapping the title and skills to standardized taxonomy like O*NET job codes). The result is an enriched candidate profile – e.g. a canonicalized current job title, a list of normalized skills, and other metadata that will improve matching.
Job Search Request: The frontend then submits the job search request to the Job Search API (the matching service). This request typically includes the candidate’s vectorized resume or profile representation (the frontend can send raw text which the backend then embeds, or the frontend might have received an embedding from Hermes if precomputed – usually the backend will handle embedding). Importantly, this API call does not immediately initiate any external provider scraping. Instead, it triggers a search within the platform’s local job cache/database to retrieve matching jobs (details on this in the next sections).
At this point, the search pipeline on the backend takes over in an asynchronous fashion. The frontend remains in control thread-wise – it does not block indefinitely on the API response. Depending on the implementation, two patterns are possible:
Synchronous Initial Response + Async Update: The /match API could quickly return a preliminary list of jobs found in the cache (within milliseconds, since it’s just a database vector query). If the user is a free user, this is the final result set (no new data will be fetched). If the user is a paid subscriber, the backend may also kick off background tasks to fetch more jobs (if needed) but return the cached results immediately. The frontend can display these immediate results and indicate that more results are loading. As the enrichment tasks complete and new jobs are discovered, the frontend receives updates (via polling or WebSocket events) and refreshes the results list with the more complete data.
Fully Asynchronous Workflow: Alternatively, the /match endpoint call can immediately acknowledge the request (or return minimal data), and the entire matching and enrichment process runs asynchronously. The frontend would then poll an endpoint or wait for a notification when results are ready. In this model, the user might see a spinner for a few seconds and then the full results appear once all background work is done. This approach simplifies the logic (only one final result set), but it introduces a slight delay even for cached results. To preserve a snappy UX, we favor the first pattern: return cached matches instantly, then update incrementally.
In either case, the Celery task queue is used in the backend to manage the asynchronous jobs (for any heavy lifting beyond the initial DB lookup). The frontend’s role is to trigger these tasks via API calls and then handle responses as they arrive. No backend service calls another service behind the scenes without the frontend’s initiation – for example, the Job Search service will not directly call the Hermes service or the Imaginator service on its own. Each service does its job in isolation when asked, and the client coordinates the flow.
Avoiding Unprompted Background Jobs: Critically, this orchestration model enforces that no background polling or scheduled scraping occurs. There is no “cron job” pulling new jobs periodically, and no automatic “fresh search” that runs without a user request. This is a deliberate constraint to limit costs. All scraping of external sources (the expensive operations) happen on-demand. For instance, if no user ever searches for “Data Scientist in NYC”, the system will never call HasData’s API for that query. If a user does search it and is a subscriber, only then do we fetch those results (and cache them for reuse). This on-demand model ensures maximum relevance (we fetch what users actually need) and cost control (no wasted credits on unused data).
Frontend Feedback and Control: As tasks execute, the Job Search module will emit progress and credit usage info back to the frontend. For example, if the system had to call the HasData API to find new jobs, it will report how many API credits were consumed for that action. The frontend could display a tiny message like “(Fetched 50 new jobs, 5 credits used)” to a power user or log it for internal monitoring. Similarly, if rate limits or budget constraints prevent a live search, the backend can inform the frontend (e.g. returning an error or a flag) so the UI can gracefully notify the user (for instance, “Live search unavailable, showing cached results only”). By keeping the frontend in the loop at all times, we maintain transparency about system actions and costs.
In summary, the orchestration model is user-driven and asynchronous: the frontend breaks the overall job-finding process into service calls (Document Reader -> Hermes -> Search -> etc.), and uses the results of one step to decide the next. The backend performs intensive tasks in the background, allowing the frontend to stay responsive. This architecture avoids any uncontrolled chain reactions between services, making the system easier to reason about, debug, and scale independently.
Discovery Flow (On-Demand Job Retrieval via HasData & Redis Controls)
“Discovery” refers to finding new job listings from external sources (primarily via the HasData APIs) to augment our local cache when a user’s query cannot be satisfied with cached data alone. In the unified architecture, discovery is only triggered under specific conditions and always in response to a paid user’s search action.
Trigger Conditions: After the initial search against the cached jobs, the system evaluates the result quality for paid users. The plan imposes a simple rule: if the top cached matches have a low relevance score (< 60 out of 100) and fewer than 100 cached jobs were found for the query, then trigger a discovery phase. This threshold ensures we only spend credits when the existing data is clearly insufficient (e.g., niche query or a new type of candidate with no good cached matches). If cached results are plentiful and reasonably relevant, we skip discovery to save cost, even for subscribers. Free users never trigger discovery (they will just see whatever the cache has, even if it’s empty or low-quality).
Discovery Process: When discovery is needed, the frontend (or the job search API on behalf of the frontend) will enqueue a Celery task to perform the job lookup via HasData. This task flows as follows:
Query Construction: The task prepares a search query for the HasData Listing API. This query can be derived from the user’s resume/profile – for example, using the candidate’s target job title or top skills and location to form keywords. Hermes’ output can assist here (e.g., if Hermes says the candidate’s canonical title is “Software Engineer” and location “Seattle, WA”, the query might be “Software Engineer Seattle”). If the system supports keyword search, it might also use relevant terms extracted from the resume.
HasData Listing API Call: The task makes a request to the external HasData Listing API (through the internal API Gateway, see below). The Listing API typically returns a page of job listings – each with basic info like job title, company, location, a short snippet or summary, and a URL/ID for the full description. Importantly, this call is relatively cost-effective: HasData might charge, say, 5 credits for up to 50 listings in one request[10]. We maximize the value by retrieving as many listings as possible in one go. The result is a bulk list of potential jobs related to the query.
Rate Limit & Concurrency Control: The call to HasData goes through the Redis-backed token bucket mechanism. This ensures that at most the allowed number of requests run concurrently and that we don’t exceed any provider rate limits. For example, if 10 discovery tasks fire in a short window, the token bucket might queue some to ensure only up to 15 run at once (matching HasData’s 15 concurrent request limit). The token bucket allows brief bursts (up to 15 simultaneous calls) but throttles sustained high rates, and it coordinates across all workers by storing counters in Redis[11]. In practice, the Celery worker performing the call will block if necessary until it obtains a token from Redis indicating it can proceed, then call the API, then release the token or decrement available tokens.
Parse and Store Listings: The response from HasData is parsed. For each job listing returned, the task checks if we already have this job in our Postgres jobs table (using a unique ID or URL as key). Duplicates are skipped (this prevents re-processing the same job repeatedly). New listings are inserted into the database with a status like discovered (meaning we have basic info but not full details yet). The insert will include all the metadata from the listing: title, company, location, snippet, posting date, etc., as well as a timestamp for when it was discovered.
Initial Filtering: Before we commit to further processing, we apply any hard filters to these new listings[12]. For example, if the user’s preferences require “onsite only” and the listing is remote, or if the candidate lacks the required visa and the job doesn’t sponsor, etc., we might drop those listings now. Also, if the user set a minimum salary and the listing’s salary is below that, filter it out. These criteria quickly reduce irrelevant results, saving work down the line. The filtered-out ones can either be discarded or kept with a flag (but they wouldn’t be shown to the user).
Listing Embeddings: For each new listing that passed filters, the system generates a listing-level embedding vector[13]. This is a fixed-size numeric representation of the job based on its title + company + location + snippet. It’s “compact” (e.g. 384 or 512 dimensions) and captures the semantic gist of the job posting in brief. We use a pre-trained embedding model (likely the same model used for resume embeddings, or a related model) to embed these fields. This operation can be done fairly quickly and is much cheaper than dealing with full text. The embedding is then stored in the Postgres/pgvector index for listings.
Semantic Narrowing: Now we have a set of new job vectors. We perform a vector similarity search between the user’s resume vector and these new listing vectors (this could be done by the same worker or delegated to the database via an upsert-and-query). We combine the embedding similarity score with any other quick heuristics (such as location proximity or snippet keyword matches) to rank the new jobs by relevance[14]. From this ranking, we take the top results (up to 100 candidates, for example) that seem promising for the user.
Merging with Cached Results: These newly discovered top listings are merged with the previously found cached results (if any). We then have up to 100+ jobs (some originally from cache, some new). We must ensure no duplicates (a job that was in cache and also came from the API call should be unified; likely it would have been filtered out as duplicate in step 4). At this point, we have an expanded pool of candidates for the user’s query.
After the discovery phase, the system proceeds to enrich the top candidates with full details (next section) before presenting them. It’s worth noting that discovery tasks are idempotent and conditional – if the same query is triggered again later, the process won’t re-add duplicates and may not even call the API if the data is still fresh. For instance, we might cache the results of query “Software Engineer Seattle” in Redis for a short interval (say 5 minutes) so that repeated searches (perhaps by the same user refining filters) don’t call HasData repeatedly. But beyond such short-term caching, the main persistence is the database itself: once jobs are in the DB, subsequent searches will find them, making external calls unnecessary.
The entire discovery flow is governed by cost-awareness and rate limits. The use of Redis as a coordination layer is twofold: it acts as a cache to avoid redundant queries in a short time span, and as a store for rate limiting tokens and credit counters. By the end of discovery, we have spent some number of HasData credits (e.g. 5 credits for one listing API call) – this is immediately recorded in our credit tracking (the next section details this). If the budget was close to exhausted or the rate limit was hit, the gateway might have denied the call; in that case the discovery task would abort and the user would simply get whatever cached results existed (with perhaps a notice that live search is unavailable).
In summary, the Discovery Flow is an on-demand pipeline that carefully pulls in fresh jobs when needed, while using Redis and the gateway to enforce the Startup Plan limits on concurrency and monthly usage. By caching every discovered job, we gradually build a rich local repository that reduces the need for future external calls.
Caching Policies and Expiry
Caching is central to our architecture’s cost efficiency. We cache job data at multiple levels to maximize reuse and minimize external API calls:
Primary Job Cache (Postgres Database): All job listings and their details are stored in a PostgreSQL database (which also hosts vector indexes via pgvector). This acts as a long-term, persistent cache of jobs seen by the system[15][16]. When a user searches, we first query this database for matches; if we find relevant jobs that were discovered in the past, we can serve them immediately without calling external APIs. Each job record includes metadata (title, company, etc.), the snippet, any structured fields, and possibly the link to full description stored in S3 (if enriched). It also stores timestamps: when it was last discovered and last enriched.
In-Memory & Redis Caches: On top of the database, we employ faster caches for frequently accessed results. For example, recent search results or popular jobs might be cached in Redis for a few minutes to speed up identical repeat searches[17]. Also, in-memory caches on the application server can hold very hot data (e.g., caching the embedding of the last resume searched to avoid recomputation on minor changes). These caches have short TTLs (on the order of minutes) and serve to reduce load on the database and computation, but they are not relied upon for long-term storage.
Content Addressable Caching: We use a content_hash for job descriptions to avoid processing the same content twice[18]. For instance, if the same job posting (identical text) appears via two different sources or queries, we detect that and avoid re-enriching it. This hash is stored in the job record. If a new discovery finds a job whose content hash matches an existing entry (even if the ID differs), we know it’s a duplicate and skip it. This prevents paying twice for the same data.
Expiration Policy (1.5-Month TTL): To ensure the cache stays relevant and to avoid serving stale or expired job posts, we enforce a 45-day expiration window for cached jobs. Any job older than ~1.5 months from its last update is considered expired: - We will exclude jobs older than 1.5 months from search results. For example, the match query can add a filter WHERE last_discovered_at > NOW() - INTERVAL '45 days' so that very old listings don’t get returned[19]. This keeps results fresh (jobs older than ~6 weeks are likely filled or closed). - We also schedule a cleanup task (see Maintenance section) that periodically purges or archives jobs past this age from the database. This reclaims storage and ensures we don’t endlessly accumulate data. Before deletion, we might mark them as inactive or move to an archive table for logging, but they won’t be used in active matching.
Cache-Only for Free Users: For free-tier users, the cache is effectively a read-only data source for queries. The system will never fetch new jobs on behalf of a free user. This means the cache must do the best it can. If a free user’s search finds nothing in the cache, the result will simply be empty (or maybe a message suggesting they upgrade to search live jobs). We accept that free users might get zero or suboptimal results if their query hasn’t been seen before. However, over time as other searches populate the cache, the free user experience improves. The expectation is that popular queries and common resume profiles will be well-covered by cached data (especially fueled by paid user activity). This cache-only approach for free users is a key cost-control: it guarantees free usage doesn’t incur incremental API costs.
Cache Refresh and Staleness: Within the 45-day window, we generally avoid refreshing a job’s data from the source more than necessary. The assumption is that job postings don’t significantly change once published (aside from being taken down when filled). Thus, our enrichment of a job is one-time. We do set a shorter refresh TTL for content in case we want to update a job’s details. For example, the architecture refinement draft suggested a 7-day TTL for enriched content[18] – meaning if a job is still in the system after a week and being considered again, we might re-call the API to see if the description changed or confirm it’s still active. This is an optional optimization; in practice, it may be sufficient to trust the 45-day window and assume if it’s listed, it’s active. If we choose to implement content refresh, we would store last_enriched_at and only enrich again if that timestamp is older than 7 days (and still within the 45-day life of the job)[20].
Caching of Search Results: Besides caching the jobs themselves, we can also cache match results for a given user profile for a short time. For instance, if the same user submits the same resume twice in an hour, the second time we can directly return the previous top results (since the cache of jobs hasn’t changed much) to further reduce latency and load. This can be done via a hash of the resume content as a key in Redis mapping to the last result list. However, because our matching is very fast and because new jobs might come in, this is a minor improvement; the primary caches of job data and embeddings already yield most benefits.
In summary, the caching strategy ensures that most user queries hit the cache and return quickly without external calls. We expire cache entries after 1.5 months to keep data fresh and to control database size. The cache policy is aggressive in reuse but conservative in refresh – get as much value as possible out of each credit spent by serving multiple users, but do not show data that is likely outdated. By tuning the cache TTL and using content hashes to avoid redundancy, we target a >90% cache hit rate and an 80%+ reduction in external API calls[2][21].
Vector Search and Ranking Layers
To efficiently match users to relevant jobs, the system employs a multi-layered search and ranking process that combines vector similarity with heuristic scoring. This approach balances recall (finding a broad set of potentially relevant jobs) with precision (accurately ranking the best jobs at the top). It involves two main stages of vector-based retrieval followed by re-ranking of results.
1. Resume Embedding: When the user’s resume or profile is received (after Document Reader and Hermes processing), the Job Search service first transforms it into a numerical embedding vector. This is done using a pre-trained language model (e.g. a transformer model fine-tuned for job/candidate matching). The result might be a 384- to 768-dimensional vector that represents the semantic content of the resume. This vector captures the candidate’s skills, experiences, and preferences in a way that can be compared to job vectors.
2. Initial Vector Search (ANN Retrieval): Using the resume vector as a query, the system performs a nearest-neighbor search in the space of job listing embeddings. Each job in the database has a precomputed listing-level embedding (as described in Discovery Flow) representing its title, company, snippet, etc. We use an approximate nearest neighbor index (like HNSW or IVF) on these vectors for fast similarity search[22][23]. The result is a set of jobs whose listing embedding is most similar to the resume embedding – typically we retrieve a few hundred candidates for thoroughness (e.g. the top 500 by cosine similarity distance)[14]. This vector search provides a broad recall of potentially relevant jobs in semantic terms, far more flexible than keyword matching.
3. Preliminary Scoring and Narrowing: The retrieved candidates are then scored and filtered using additional criteria: - We compute an ATS-based match score for each job. This score simulates how well the job aligns with the candidate’s profile on structured features: e.g., required vs. offered skills, years of experience, education level, job title seniority match, location match, etc.[24][25]. A machine learning model (such as a Random Forest or SVM classifier) could be used for this scoring[26], producing a normalized score between 0 and 100. - We also consider freshness (newer postings get a slight boost) and any user-specified preferences (like preferred industry or company size if available). - The semantic similarity from the vector search can be combined with the ATS score to produce a composite ranking score. For example, we might take a weighted sum: FinalScore = 0.7*(ATS_score) + 0.3*(Semantic_similarity_score) (with appropriate normalization), or even train a model to combine them. - We then sort the candidates by this score and take the top N – for instance, the top 100 jobs[27]. This step dramatically narrows down the list from hundreds to a more manageable number of high-quality matches.
By the end of this stage, we have the top ~100 jobs that seem best for the user, based on the information readily available (listing data and basic features). If all these jobs were already enriched (full details available), we could actually stop here and return the final top results. However, often many of these 100 will not yet have full descriptions fetched, especially the newly discovered ones from HasData. To ensure the final ranking is as accurate as possible, we proceed to the next stage, which deals with enrichment and full-text analysis.
4. Enrichment and Full-Text Vector Re-Ranking: For the top N jobs, the system now fetches their full descriptions (if not already cached) and uses that to refine the ranking: - Each of the top jobs that has enrichment_status = 'none' (no full text yet) is sent to an enrichment task (detailed in the next section). Once retrieved, we generate a full-text embedding for the job[28][29], using a larger model or more expansive embedding (e.g. 1536 dimensions covering the entire job description). This embedding gives a deeper semantic representation of the job. - We then recompute similarity between the user’s resume vector and each job’s full-text embedding. This is more accurate than the listing snippet similarity we used earlier, because it considers the entire job responsibilities, requirements, etc. - The final match score for each job can be adjusted in light of the full-text similarity. In many cases, this might shuffle the order of the top 100 slightly – for example, a job whose snippet looked somewhat relevant might turn out, in full text, to be a near-perfect match and thus should rank higher. Conversely, some jobs might drop if their full description reveals misalignment with the candidate. - After incorporating the full-text info, we re-sort the list of top candidates. At this point, we can confidently pick the top results (say the top 20 jobs) to present to the user as the final matches. These will be the ones with the highest combined scores post-enrichment.
This two-level vector search (listing-level and then full-text) is the “dual vector search” strategy. It allows us to first cast a wide net cheaply, then apply heavy computation only on the short list of promising jobs. The design ensures we spend our “AI budget” (in terms of API credits and compute) where it matters most: we only enrich and fully analyze the top ~100 jobs for a query[30], not thousands. This is exactly in line with the cost-saving strategy: discover broadly, enrich narrowly[30].
Performance Considerations: The vector search in the database is extremely fast (millisecond-range for HNSW index queries) and can handle large volumes of jobs (hundreds of thousands) with ease[31][32]. By pre-indexing embeddings, we avoid iterating over all jobs per request. The subsequent scoring and ranking of a few hundred jobs is also quick (sub-second), especially if using vectorized operations or simple ML models. The slowest part is enrichment, which we handle asynchronously. Overall, the user should experience initial results in under ~1 second (from cache), and final refined results within a few more seconds if enrichment is needed, which is acceptable.
Example: Suppose a user’s resume is for a “Data Analyst”. The resume embedding might nearest-neighbor to 500 jobs that include keywords like “data”, “analyst”, “analysis”, etc. Among those, our ATS scoring might favor those requiring SQL, Python, which the candidate has, narrowing to top 100 that fit best. After enrichment, we might find that 5 of those jobs specifically mention the candidate’s niche expertise (say, A/B testing or Tableau) in the full text, boosting their rank. The final top 20 reflects those adjustments, ensuring the user sees the jobs most likely to be a true match.
In conclusion, the ranking pipeline ensures relevance and efficiency by layering a coarse semantic search with fine-grained analysis on the finalists. This dual approach is both more accurate than a single-pass method and significantly more cost-effective than trying to score every job against every resume in real-time[33][7].
Enrichment Pipeline and Asynchronous Re-Ranking
“Enrichment” is the process of retrieving the full job details and augmenting the job records with rich data (full description text, structured fields, etc.). It is an expensive step (in terms of API credits and time), so we do it asynchronously and selectively. The enrichment pipeline works hand-in-hand with the re-ranking logic described above, effectively providing the data needed for that final ranking refinement.
Selection of Jobs to Enrich: We do not enrich every job listing in the database – only those that are likely to be shown to users. In practice, as soon as a job enters the top results for any user’s query (and the user is a subscriber), that job becomes a candidate for enrichment. In the pipeline above, we identified the top N (e.g. 100) jobs for the query. For each of those jobs, if it doesn’t already have full text, we will enrich it now. This “just-in-time” approach ensures we spend credits only on jobs that have proven to be relevant enough to surface.
Enrichment via Celery Tasks: The system spawns parallel Celery tasks to perform enrichment for each selected job. We can leverage a Celery group construct to fan out up to N tasks concurrently (with an upper limit to respect concurrency limits)[34][35]. Each task does the following:
Job API Call: Call the HasData Job Detail API for the given job (via the API Gateway). We use the job’s unique identifier or URL obtained from the listing to request its full description. This API typically returns the full job posting text (responsibilities, requirements, etc.) and sometimes structured data like benefits, seniority level, etc. Each such call might cost a few credits (often 1 credit per job or similar). The gateway monitors these calls; if our monthly credit pool is low or the circuit is open, it might refuse the call (the task would then mark enrichment failed or retry later).
Fallback Scraping: If the Job API call fails (e.g. perhaps HasData doesn’t have that job’s details cached, or the API returns incomplete data), the task falls back to a web scraping API provided by HasData[36]. Using a generic scraping endpoint (with predefined extraction rules or simply retrieving raw HTML) ensures we still get the data albeit at potentially higher cost or complexity. The fallback is only used when absolutely necessary. If both the Job API and scraping fail, the task records an error and gives up on that job for now.
Store Raw and Parsed Data: Upon getting the full job content, the task stores it persistently. We utilize S3 (or another blob storage) to save the raw payload – for example, the HTML or JSON from the provider – for audit and potential re-parsing[37]. We also extract the plain text from it (if needed) and store that either in S3 or directly in the database in a text column. The job record in Postgres is updated with a pointer (S3 key) to the full text, a content_hash of the text (for deduping as noted), and timestamps: enriched_at now set to current time[38]. The status of the job is marked enriched. All this makes the job’s full data available for future queries (it’s now effectively part of our cache).
Full-text Embedding Generation: The enrichment task then calls the ML model to generate the full_text_embedding for this job[28]. This can be done using the same Celery task or a sub-task on a specialized queue (depending on performance needs). The embedding generation uses a more powerful model (or more comprehensive input) than the listing embedding. We then update the Postgres record with this vector (and also insert it into the vector index for similarity search). With this, the job is fully indexed for semantic matching on complete text.
Post-Enrichment Hook: Once the job is enriched, the system can trigger a post-processing step (Celery chord callback or simply let the orchestrator know). If this job was enriched as part of an ongoing user query, we then proceed to update that query’s results.
Often we will enrich multiple jobs in parallel. We control concurrency to not exceed the limit (e.g., allow up to 10-15 enrichment tasks to run simultaneously, aligning with the API’s 15 concurrent calls limit). Celery workers on the enrichment queue are configured with a concurrency setting (like --concurrency=10) to enforce this at the worker level too[35]. They also use acks_late=True so that if a worker dies mid-task, the task will be retried elsewhere without losing the message[39], ensuring reliability.
Asynchronous Re-Ranking: Because enrichment happens asynchronously, the system must handle the fact that the final ranking is only determined after those tasks finish. There are two timing scenarios: - If using incremental update UX: The initial results (from listing data) are shown, then as enrichment completes, the frontend receives updated scores/details. The Job Search module can recalc the similarity for each job that got enriched and then send the new ranking info to the frontend. In practice, a job’s match_score might improve (or occasionally decline) after enrichment; the frontend can reorder the list or simply update each job card with more info (like showing full description or “match reasons”). The final set of jobs might remain the same set of top 20, just in a slightly different order. - If waiting for final results: The backend, after kicking off enrichment tasks, waits until they all complete (or a majority complete with a timeout). Then it performs the re-ranking internally and returns the final ordered list to the frontend in one response. The user only sees the final ranking. Any jobs that failed to enrich (due to error or timeout) can still be included based on their earlier partial score, but we might penalize them slightly or mark that details are unavailable. Usually, though, within a few seconds, most top jobs will be enriched.
Either way, enrichment improves the result quality. It also adds value to the user by allowing us to show more details. For example, on the search results page, we initially show the snippet (short summary) for each job. After enrichment, we can display a more informative snippet or highlights from the full description, and we can provide “why match” explanations (e.g., specific skills matched) because we have the full text. These explanations might come from comparing resume and job full text, potentially leveraging the Hermes analysis (for skills matching) to list out common skills or requirements met[40].
It’s important to note that enrichment tasks respect the idempotency and TTL rules. If a job was enriched in the last X days, we don’t enrich it again; the task will notice that last_enriched_at is recent and skip (immediately succeed)[20]. This prevents wasteful re-fetching of the same data within a short period. Also, if two users around the same time trigger enrichment for the same job, only one actual API call is made – one task will do it, the other will detect that the job became enriched and do nothing. Coordination via the database and task queue ensures we don’t duplicate work.
Outcome of Enrichment: After enrichment and re-ranking, the final set of jobs is ready. Each job object has: - Full job description (or a key to retrieve it from storage) if enriched. - A final match score and possibly an ordered list of match reasons (e.g., “Skill match: Python, SQL; Experience match: 5 years required, 6 years in resume”) for transparency[41]. - All the metadata needed for display (title, company, location, salary if any, etc.).
These are returned via the API to the frontend. Typically, we cap the number of results returned (e.g., top 20) to avoid overwhelming the user and to meet UI constraints[42][43]. The response format is consistent, always returning a fixed number of top jobs with their scores and details.
In summary, the enrichment pipeline is a controlled asynchronous workflow that brings in high-fidelity data for the best jobs and feeds it back into the ranking. It ensures that costly operations (detailed scraping and ML on full text) are only applied to a small subset of data that truly needs it, aligning with our cost-aware mandate. By doing this asynchronously, we keep the user interface responsive and allow parallelism to speed up completion. The combination of enrichment + re-ranking gives the benefit of a thorough search (as if we had all jobs’ data) without the prohibitive cost of analyzing every job in the market in real-time.
Credit Tracking and Rate Limiting
To enforce budgetary constraints and prevent abuse of external APIs, the unified architecture includes a robust credit tracking and rate limiting system, centered around an internal API Gateway service and Redis.
Central API Gateway: All outbound requests to third-party APIs (like HasData) funnel through a single gateway service[6][44]. This gateway is a lightweight FastAPI-based microservice that acts as a proxy and gatekeeper. Celery tasks or backend services do not call HasData directly; instead, they make an internal request to the gateway (e.g., POST /proxy/hasdata/listing or GET /proxy/hasdata/job?id=XYZ). The gateway then injects the appropriate API keys and forwards the request to HasData[45]. By centralizing this, we achieve: - Key Management: API keys for HasData (and any other provider) are stored securely in the gateway, not scattered across code. Workers never see the keys, enhancing security[46]. - Global Rate Limiting: The gateway implements the distributed token bucket algorithm to throttle calls[11]. A token bucket is configured per provider (in this case, HasData’s endpoints). We set the bucket capacity and refill rate according to allowed throughput. For example, capacity = 15 (max concurrent calls) and a refill rate that permits perhaps 15 requests per second (if HasData had a QPS limit; if not, the main limit is concurrency). If a request arrives and the bucket has no tokens, the gateway will respond with HTTP 429 Too Many Requests, indicating the caller should retry after a short delay[47]. All gateway instances use Redis to share token counts, so scaling the gateway won’t break the rate limit – the limit is truly global. - Real-Time Credit Counting: The gateway inspects every response from HasData to extract how many credits that API call consumed[48]. (HasData’s API responses include metadata on credit usage per call, or we calculate it from known costs.) The gateway then updates a Redis counter for HasData credits, e.g., INCRBY hasdata_credits <cost>[48]. This counter accumulates usage for the current billing period (month). - Monthly Budget Enforcement: With the credit counter, we can enforce the monthly quota of 200k credits. The gateway is configured with the monthly budget and can compare the used credits against it. We set up alerts at 70% and 90% of the budget[49] – for example, when hasdata_credits exceeds 140k, send a warning to Slack or PagerDuty, and at 180k, send a critical alert. If the counter would exceed 200k (e.g., a new request would push it over), the gateway will block the request outright (circuit breaker). - Circuit Breaker: The gateway implements a circuit breaker pattern for the HasData integration[47]. If we hit the budget limit (200k) or if HasData’s API starts consistently failing (e.g., high error rate), the circuit “opens” for that provider. While open, all attempts to call that API are immediately rejected (429) without even trying to forward. This protects the budget (no additional credits are spent) and allows the system to degrade gracefully when we’re out of quota or when the provider is unstable. The circuit can automatically half-close (allowing a test request) after some cooldown or be manually reset at the start of the next month.
All these mechanisms ensure we never overspend beyond what’s allowed and we respect external constraints. The Redis token bucket and counters provide a central view of usage.
Integration with Celery Tasks: The tasks performing discovery or enrichment simply call the gateway and handle responses. If they get a 429 from the gateway (rate limit hit or circuit open), they will catch that and retry after a delay (using Celery’s retry with exponential backoff)[50]. This means if too many tasks hit at once, they’ll gracefully queue up. If the circuit is open due to budget, tasks might log an error and abort because no retry will succeed until next period. The system could also downgrade the user experience in that case (e.g., notify that live search is unavailable).
Credit Usage Feedback: The gateway also provides an endpoint /status/credits that returns current usage vs budget[51]. The frontend or an admin dashboard can call this to display usage. Additionally, after each search that involved external calls, the backend can return in the response how many credits were used for that operation. For instance, the result payload to the frontend might include a field "credits_used": 5 if that search triggered one HasData call costing 5 credits. This information can be surfaced to users (especially admin users or in subscription management interfaces) to increase transparency. For paid users, we might not show the raw credit but could use it internally to gauge how expensive certain user queries are.
Handling Free vs Paid in Budget: Because free users don’t invoke external calls, they don’t directly affect the credit counters. Their searches are served from cache at essentially zero cost. Paid users do consume credits, and the system might even attribute usage per user if needed (for example, to prevent a single subscriber from exhausting all credits on repeated searches – we might implement a fair usage policy). Currently, our focus is on global usage, but in the future we could track credits per user or per organization if offering enterprise accounts.
HasData Plan Limits: The specific numbers (200k credits/month, 15 concurrent) are baked into the rate limiter config. For example, the token bucket might allow 15 tokens instantly (for concurrency) and maybe refill at 0.05 tokens per second if we wanted to smooth out to ~200k/month (this part is tricky because 200k/month isn’t a simple per-second rate – we likely handle the monthly budget via the counter instead). Essentially, concurrency is handled by tokens, monthly total by the counter & circuit breaker. If HasData also has a daily or per-minute limit, we could incorporate that as well by adjusting token refill rates or adding secondary limits (not mentioned, so presumably not).
Other Providers: While HasData is the main one, our gateway approach extends to any external API (e.g., additional scraping services). Each provider can have its own token bucket and counter in Redis. This unified approach simplifies cost management for the entire platform.
In conclusion, Credit Tracking and Rate Limiting is the safety net of the architecture. It ensures that no matter how many users are on the system or how enthusiastic they are, we cannot accidentally run up a huge API bill or violate provider limits. The system will throttle and eventually refuse requests as limits are approached, favoring a degraded service (e.g., “only cached results available”) over an outage or financial overrun. All of this operates transparently, with the frontend kept informed so it can adjust the user experience accordingly. These measures, though mostly behind-the-scenes, are crucial for a “cost-aware” architecture.
Free vs. Paid User Flows
The unified architecture explicitly differentiates between free users and subscribed (paid) users in how job searches are handled. This split is designed to offer a baseline service at zero marginal cost for free users while providing an enhanced experience for paying customers that justifies the credits expenditure. Below we outline the distinct flows:
Free User Flow (Cache-Only Search)
Resume Upload & Analysis: The free user uploads their resume and the system parses it (Document Reader) and may run Hermes analysis just like for any user. These steps are inexpensive (they use internal services) and are allowed for free users.
Job Search in Cache: When the frontend calls the Job Search API for a free user, the backend only searches the local database of jobs. It performs the vector search and initial ranking among cached jobs exactly as described in the Vector Search section. This operation is fast and cost-free (aside from our own compute), so it’s permitted for free tier.
No Discovery Trigger: Regardless of the result quality or count, the system will not call the HasData API for a free user. Even if the best match score is 20 and there are only 5 jobs found, the free user’s query stops here. The rationale is to avoid incurring external costs for users who are not contributing revenue. The application may choose to communicate this limitation (e.g., “Upgrade to Pro to search 1000+ live jobs”).
Return Results: The free user receives whatever matches were found in the cache. If the cache had plenty of relevant jobs (perhaps thanks to prior paid searches or previously cached popular listings), the free user could still get a good experience. If not, the user might see few/no jobs. We might consider features like “popular jobs” or default results in such cases, but that’s a product decision. Technically, the result set is limited by the cache content.
No Enrichment for Free Results: If a job was already enriched in the cache (from some past process), the free user will see the enriched details. But we will not initiate a new enrichment for the sake of a free user. For example, if a free user happens to surface a job that’s in cache but only has snippet (not full text), we won’t spend credits to enrich it for them. They’ll just see the snippet. Enrichment tasks are only queued on behalf of subscriber searches.
In summary, the free flow is read-only with respect to our job database. It’s essentially a look-up service on our accumulated data. This means the cost per free search is near zero (some CPU for vector math and DB querying). It also means free users might not always get the most up-to-date jobs, especially in niche areas, but this is an acceptable trade-off.
Paid User Flow (Cache + On-Demand Discovery)
Resume Upload & Analysis: Same as free flow – the subscriber’s resume is processed through Document Reader and Hermes to prepare for search.
Job Search in Cache: The initial query goes to the local cache (DB) first, just like for a free user. We perform the fast vector search among cached jobs and produce an initial list of matches with scores.
Evaluate Sufficiency: The system checks the top results against the predefined thresholds: are there at least 100 results? Is the top match score (or average of top few) at least 60? If both conditions are satisfied (plenty of decent matches), then cache results are deemed sufficient. In that case, the paid user’s experience is identical to the free user’s at this point – they get the results from cache immediately. The difference is, if they weren’t great, the paid user can get more.
On-Demand Discovery: If the cache results are insufficient (too few or low scoring), the backend triggers the Discovery Flow for this query. The frontend might display a message like “Searching for more jobs...” or a spinner. Under the hood, a Celery task (as detailed earlier) calls the HasData Listing API with the query, fetches new listings, inserts them into the DB, etc. This is where the paid user is leveraging their subscription – the system is now spending API credits to find them better matches.
Merging and Enrichment: The newly discovered jobs are combined with the cached ones. Now the ranking process runs on this augmented set. The top 100 are identified, and then enrichment tasks are launched for those requiring full text. The paid user may need to wait a moment for these to complete. However, we might also choose to return the cached results immediately and then update with new ones as they come in. UX design could vary:
We could initially show, say, “5 jobs found in cache” then after a second, “+ 20 new jobs found” appear in the list dynamically.
Or simply wait 1-2 seconds and show the final combined 20 results (if the slight delay is acceptable).
Return Results: The subscriber ultimately sees a much richer result set – ideally the top 20 jobs that include both any relevant cached jobs and brand new ones discovered just for them. These results will have full details (post-enrichment) and high confidence scores. The improvement over the free experience is that even a completely novel query can yield good matches because the system actually went and fetched fresh data for it.
Subsequent Searches: If the same subscriber or another subscriber searches something similar again soon, those jobs are now cached, so the next search might not need to call HasData at all. In this way, paid user activity continuously expands the cache for everyone.
Subscriber Limits and Fair Use: Our design assumes we won’t have a single user abuse the system to call HasData excessively, but theoretically a paid user could trigger many unique searches. We rely on the global credit limit to cap overall usage. If one user somehow uses up a lot of credits, it just hastens the point where the budget runs out and then all subscribers will be limited to cache until next month (we could implement individual quotas to prevent one user from hogging, but that’s beyond scope here). The presence of the match score threshold (60) and result count threshold (100) already throttle calls: if a user keeps searching very niche things that yield nothing, each time we will call HasData – but after caching those results, repeated identical queries won’t recall it. If they search many completely different queries, each could trigger calls. However, the 200k credits/month budget should accommodate a reasonable number of searches; alerts at 70%/90% usage will inform us if usage is trending high so we can intervene (like temporarily raising thresholds or asking extremely active users to slow down).
UI Differences: The frontend can emphasize to free users that their results are limited. For instance, free users might see a banner “Showing results from our database. Subscribe to search live new listings.” For subscribers, when a discovery is happening, the UI can show a progress indicator. Also, subscribers might get additional info, like the match score or reasons, as part of a premium feature set.
Example Scenario: A free user and a paid user both search for “Go developer in Paris”. Suppose our cache has few Go developer jobs for Paris (maybe 10 mediocre matches). The free user will only see those 10 and likely not be satisfied. The paid user, on the other hand, triggers discovery: the system calls HasData, finds 50 new listings, caches them, then shows the top 20 of combined old+new (maybe now the list has plenty of good matches). The difference is clear: the paid user finds opportunities that the free user wouldn’t have seen. Next time, if any user searches “Go developer Paris”, the cache now has those jobs, so even a free user might benefit then.
In essence, free vs. paid flows trade completeness for cost. Free users get a fast response but only from existing data; paid users might wait slightly longer but get a thorough search of fresh data. This delineation ensures that external API calls (which incur costs) are only done on behalf of revenue-generating users, keeping the platform’s operating costs aligned with its business model.
Module Responsibilities
The system is composed of several specialized modules and services, each with distinct responsibilities. Below we summarize the roles of key modules, including the auxiliary services Hermes, Imaginator, and Document Reader, and how they integrate into the overall architecture:
Document Reader Service: This module handles resume ingestion. It takes an uploaded resume (PDF, DOCX, image, etc.) and extracts structured text from it. The Document Reader performs OCR if needed (for scanned documents) and parsing of resume sections (education, work experience, skills). The output is clean text or a structured JSON of the resume content. In our flow, the frontend calls the Document Reader as the first step when a user provides a resume. The result flows into the Job Search module (and Hermes) as the foundational data representing the candidate[52]. This service ensures we reliably get the candidate’s information regardless of file format, encapsulating all the complexity of document parsing.
Hermes Service: Hermes is an ML-powered resume analysis and enrichment service. It takes the parsed resume data (from Document Reader) and applies machine learning and domain knowledge to enhance it. Specifically:
Hermes uses an SVM-based model (Support Vector Machine or similar) to classify and canonicalize the candidate’s job title. For example, if a resume’s title says “Sr. Data Ninja”, Hermes might map that to “Senior Data Scientist” – a title that better matches standard job taxonomy.
It performs skill extraction and validation. Hermes can identify key skills from the text (e.g., programming languages, tools, methodologies) and cross-reference them with known skill databases. It might also categorize skills into groups or validate proficiencies.
O*NET enrichment: Using the ONET occupational database (or similar knowledge bases), Hermes can infer related skills or job roles. For instance, given a software engineer resume, it might add hidden features like ONET occupation codes, or note that this occupation often requires “analytical thinking” or “teamwork” – traits that might not be explicit on the resume but can help in matching.
Essentially, Hermes produces an enriched candidate profile: standardized title, a list of normalized skills with categories, possibly an estimated seniority level or industry classification. This enriched data is fed into the job matching process – e.g., the standardized title might be used to search for jobs with that title, and the skill list might be used for filtering or as input to the embedding.
Hermes may also output a resume embedding or features for ML scoring. If Hermes includes an ML model to predict job fit, it could produce feature vectors that our matching engine (or RandomForest model) uses to score matches[53].
Integration: The Hermes service is called after Document Reader and before/during the job search. The frontend calls it explicitly in our design, and then passes its outputs to the search API. In some cases, the backend could call Hermes internally as part of the /match, but per our orchestration, we prefer the client do it.
Job Search Module / Matching Engine: This is the core service (accessible via the /match API) that orchestrates the search, ranking, and result compilation. Its responsibilities include:
Accepting the user’s profile (resume text or embedding, plus Hermes-enriched data).
Performing the vector similarity search against job embeddings in the cache.
Applying filtering and heuristic scoring (the ATS logic) to rank jobs.
Deciding whether to trigger discovery (for paid users) based on results.
Enqueuing background tasks for external API calls (discovery and enrichment) via Celery when needed.
Aggregating results from the cache and new data, and performing the final ranking.
Packaging the top results (with match scores and reasons) into the API response.
Emitting credit usage info and any state (like “some results omitted due to budget limits” if applicable) to the frontend.
It does not handle the detailed external calls itself; those are handed off to tasks and the gateway. It also doesn’t parse resumes or generate text – those are offloaded to Document Reader, Hermes, etc. It focuses on matching logic and orchestration of sub-tasks.
Imaginator Service: Imaginator is an LLM-based content enhancement module. It is not directly in the critical search path but serves a supporting role for user engagement. When a user selects a job (or a set of jobs) they are interested in, Imaginator can help them improve their application materials. Specifically:
Imaginator takes the user’s resume (or profile) and the target job description as input, and uses a Large Language Model to generate suggestions or new text that better aligns the resume to the job. For example, it might produce a tailored summary of the user’s experience highlighting the skills that the job requires (essentially a personalized cover letter or a resume section rewrite).
It could also identify gaps – e.g., “The job asks for X which isn’t in your resume; if you have that experience, consider adding a line about it.”
The output might be presented as an “enhanced resume” or a list of recommended edits for the user to incorporate.
This service uses advanced NLP (OpenAI GPT-4 or similar) which could be costly per call, so it might itself be restricted to subscribers or even a higher tier.
From an architecture standpoint, Imaginator is called on-demand via the frontend when the user requests it (for instance, clicking “Enhance my resume for this job”). It is not invoked automatically during the search process (to keep search fast and cost down). It runs isolated from the main pipeline; the only integration is that it may need the full job description (which by the time a user views a job, we likely have in cache from enrichment) and the user’s resume text.
By including Imaginator in our system, we offer a value-add beyond search: helping users take action on the results by improving their profile or application chances.
API Gateway (Internal Cost Control Service): As detailed earlier, this module handles all outbound HTTP requests to external job sources. Its role is crucial for injecting API keys, logging usage, and enforcing rate limits. It doesn’t have complex logic beyond that; it’s a thin layer but very important for centralizing cost concerns. The Job Search tasks interact with the gateway instead of directly with external APIs.
Celery Workers and Task Queues: While not a "service" in the sense of a network API, the Celery worker pool is an integral module in the architecture. We configure multiple named queues for different types of tasks:
A discovery queue for tasks that call listing APIs (these are I/O heavy but relatively low priority).
An enrichment queue for tasks that call job detail APIs and do embedding (I/O heavy and CPU heavy, high priority for user waiting).
Possibly an ML queue for tasks like embedding generation if that becomes a bottleneck (to separate CPU-bound tasks).
Celery workers are configured with concurrency and reliability settings (ack late, retries) appropriate to their queue[54][39]. They ensure our asynchronous pipeline executes reliably.
Celery Beat (scheduler) might be used for maintenance tasks (and in legacy might have been used for periodic ingestion, but now limited to maintenance).
Database (Postgres + pgvector): The Postgres database is the authoritative store for job data. It holds the jobs table with all the fields and the vector embeddings, and is indexed to support fast vector similarity and filtering[16][23]. It also may store user profiles or search logs, but for our focus, the jobs table is key. The DB ensures data durability (even if Redis cache is flushed or workers restart, we don’t lose collected jobs). It also enforces uniqueness (so we don’t store the same job twice) and can be used to do analytical queries (like find how many jobs in cache, etc.). In our design, heavy use of Postgres necessitates good indexing and possibly partitioning if it grows, but given the expiration of 45 days, the data volume is capped by how many jobs come in 45 days (which might be a few hundred thousand at most, manageable by a single DB instance with vector support).
Redis: Redis serves several roles:
Cache store: for short-lived caches of results or intermediate data (e.g., caching queries or partial results as discussed).
Rate Limiter store: holding token buckets and counters for API usage (as part of gateway functionality).
Task broker (optional): We are using RabbitMQ as the primary message broker for Celery (for reliability), but Redis could have been a broker. We stick with Rabbit for Celery, and use Redis for fast ephemeral storage and atomic counters.
Session state or locks: If needed, Redis can be used to coordinate processes (e.g., a lock for certain critical sections, or to store user session data, etc., though not central to job search).
Monitoring & Alerting Tools: While not a module per se, we will have integration with monitoring systems (like Prometheus for metrics, or a logging system for tracking events). Alerts for budget thresholds (70%, 90%) will likely be implemented as part of the gateway or a small cron job, sending notifications via an alert service or email[4]. This ensures the ops team is aware of cost status.
Each of these modules is decoupled and communicates through well-defined interfaces (mostly HTTP APIs or task queues). The frontend orchestrator ties them together by invoking them in sequence, which means each module can be developed, scaled, and maintained somewhat independently. For example, if we need to improve resume parsing, we update the Document Reader without touching the matching logic. If we find better matching algorithms, we tweak the Job Search module or Hermes, without affecting the gateway or document parser.
This modular breakdown also helps in scalability: we can scale out the Job Search API servers, the Celery workers, the Document Reader instances, etc., according to load. The free/premium logic mainly lives in the Job Search module (that decides whether to call external APIs), and the gateway ensures we don’t exceed usage.
In conclusion, each module has a clear responsibility: - Document Reader -> get resume text. - Hermes -> make sense of resume (skills, title, enrichment). - Job Search -> find matching jobs (using DB and orchestrating any extra work). - Imaginator -> help user post-search with AI-generated improvements. - Gateway -> guard external calls (cost/rate limit). - Workers/Tasks -> execute heavy async jobs (discovery, enrichment). - Storage (DB/S3) -> persist data for reuse. This separation of concerns follows best practices and positions the system for easier maintenance and future extension (e.g., adding another external job source simply means adding new gateway endpoints and tasks, without altering the core logic).
Cleanup and Maintenance Tasks
Even with an asynchronous, on-demand system, some background maintenance is necessary to keep the system healthy and performant. The following cleanup and maintenance tasks are part of the architecture:
Expired Job Purge: As mentioned, a scheduled task will run to remove or archive jobs older than 1.5 months. This could be a daily Celery Beat task that runs a simple SQL DELETE for jobs with last_discovered_at < now() - interval '45 days'. If we want to be cautious, we might move them to an expired_jobs table or dump to S3 before deletion, but since they’re outdated data, deletion is fine. This keeps the primary jobs table lean and query performance high.
Stale Enrichment Cleanup: If any jobs are marked enrichment_status = 'queued' or 'failed' for a long time (meaning something went wrong and they never got enriched), another periodic job can clear those flags or retry failures. We don’t want jobs stuck forever in a half-processed state. For example, if enrichment failed due to a transient issue, a nightly job might retry it up to a limit. If it consistently fails or the job is now old, we mark it as failed and it will eventually expire out.
Redis Cache Eviction: Our Redis caches (for queries or short-term job caches) have TTLs set, but it’s good to ensure the configurations are in place to evict keys when memory is full or after TTL. We should monitor Redis memory. A maintenance task could log the cache hit rate and size, and possibly flush or trim caches if needed (though ideally TTLs handle it).
Credit Counter Reset: The hasdata_credits counter in Redis needs to reset each month (assuming the 200k budget is monthly). We can schedule a job on the first of each month to reset the counters (or subtract the monthly budget from the counter, etc.). Alternatively, the gateway could calculate usage relative to the start of the month without a reset (storing the start time and using a separate counter for each month). But a simple approach is a cron job that sets hasdata_credits = 0 at midnight on the first. We’ll coordinate this so that any in-flight usage around that time is accounted properly (maybe disable external calls for a few minutes during reset to avoid race conditions).
Monitoring and Alerts: A small maintenance loop in the API Gateway will periodically check the credit usage and error rates. For example, every 5 minutes, check if hasdata_credits / budget > 0.7 and if so, ensure a 70% usage alert has been sent (and not repeatedly spam if already sent). Also check if any circuit breakers are open and log that status or send notifications. This helps catch issues early (e.g., if we see usage spiking unusually, maybe some process is misbehaving).
Search Index Optimization: Over time, as we add and remove a lot of vectors in pgvector indexes, there might be fragmentation. We should periodically reindex or vacuum the vector index for efficiency. This could be done during off-peak hours via a DB maintenance task. Because we’re continuously adding and deleting, ensuring the index remains performant is important. Postgres autovacuum may handle some of it, but a scheduled REINDEX or pgvector index maintenance command might be warranted weekly.
S3 Cleanup: If we store raw HTML/JSON of job postings in S3 (for audit or potential re-parsing), those could accumulate. We might decide to remove S3 objects for jobs once they expire from the database. A background job could list objects older than 45 days and delete them, to save storage costs. Alternatively, we can apply an S3 lifecycle rule to auto-delete objects after X days. The full-text JSON and raw HTML for expired jobs likely aren’t needed beyond that timeframe.
Backup and Recovery: Regular backups of the Postgres database should be taken (either via snapshot or logical backup). This isn’t a “task” within our app architecture per se, but part of maintenance. We should also have a process to recover or rebuild the job index if needed. Since the data is ephemeral (45-day window), a catastrophic loss isn’t the end of the world, but we’d rather not lose it. We could recreate some of it by re-scraping, but better to back up.
Updating Machine Learning Models: Occasionally, we might update the embedding model or the Hermes SVM model or similar. Such updates should be rolled out carefully. If we update the embedding model, we might need to re-embed all stored jobs with the new model. That’s a heavy operation. We could plan a maintenance task to do that gradually (recompute embeddings for existing jobs in batches, updating the index). Similarly, if Hermes’ logic for skill extraction improves, we might want to re-run it on stored resumes or adjust how we use its data. These are more ad-hoc maintenance tasks triggered by upgrades.
Logging and Analytics: We will continuously log search queries, number of results, latency, etc. A separate process might aggregate these logs for analytics – e.g., how many searches per day, what % of searches triggered external calls, cache hit rate, etc. This isn’t a cleanup but a monitoring task that helps us tune the system. For instance, if we see that 95% of searches are served from cache and only 5% trigger HasData, we’re likely within budget. If it’s the opposite, we may need to adjust thresholds or increase budget.
Celery Task Queue Health: Monitor the Celery queues for backlog. If some tasks are piling up (say enrichment tasks not being processed fast enough), we either scale workers or see if there’s a stuck issue. A watchdog task could detect if a job has been in 'queued' state for too long and alert or clear it.
All scheduled tasks (except the credit monitoring which can be inside gateway) will be managed by Celery Beat or an external scheduler. Importantly, none of these maintenance tasks involve heavy external API usage (except possibly re-embedding jobs which is local ML compute). So they do not violate our “no background scraping” rule – they’re about internal housekeeping. The only overlap is monthly credit reset and such, which is fine.
We will document these maintenance jobs in our runbook so the ops team knows what runs when. For example: - Purge expired jobs: daily at 3 AM. - Reset credit counter: 1st of month 00:00. - S3 cleanup: weekly on Sunday. - etc.
By performing these maintenance tasks, we ensure the system remains clean, efficient, and within operational limits over time. Neglecting them could lead to degraded performance (too much data to search through), inaccurate results (expired jobs showing up), or cost issues (stale data hanging around and potentially being enriched unnecessarily). Thus, they are an essential part of the unified architecture’s lifecycle.
Conclusion: This Unified Plan specification has outlined a cost-aware, asynchronous architecture for job ingestion and search. It integrates user-driven triggers, multi-stage data processing, smart caching, and strict rate/cost control. The design addresses prior shortcomings by eliminating uncontrolled background processes and moving to a pre-computation model that works within external API limits. Free and paid users are served in a manner aligned with cost expenditure, and auxiliary modules (Hermes, Imaginator, Document Reader) enrich the experience without breaking the modular, front-end-orchestrated approach. The result is a robust system that delivers relevant job matches efficiently while safeguarding the platform’s budget and scalability.
[1] [4] [5] [6] [7] [11] [18] [19] [20] [22] [30] [31] [32] [33] [34] [35] [38] [39] [44] [45] [46] [47] [48] [49] [50] [51] [54] Job Ingestion Architecture Refinement.md
file://file_000000009a6871fbbae19bfcf313761d
[2] [3] [8] [9] [17] [21] [26] [40] [41] [42] [43] [52] [53] ARCHITECTURE_AND_TECHNIQUES_SUMMARY.md
file://file_000000008dbc71f58681e7168d2c1e7c
[10] [12] [13] [14] [15] [16] [23] [24] [25] [27] [28] [29] [36] [37] Job_ingestion_semantic_search.md
file://file_00000000b598722fad37cf76585478c9


---

*Generated from: Unified Plan for a Cost-Aware, Asynchronous Job Ingestion Architecture.pdf + Unified Plan for a Cost-Aware, Asynchronous Job Ingestion Architecture.docx*
